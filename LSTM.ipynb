{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uuaK1662RZzt"
   },
   "outputs": [],
   "source": [
    "import hazm\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import typing\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel, BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordLevelTrainer, BpeTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bd-BHsnxbL9"
   },
   "source": [
    "# **General settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0EMYXYZWvoMQ"
   },
   "outputs": [],
   "source": [
    "TRAIN_TOKENIZERS = False\n",
    "\n",
    "WORD_TOKENIZER_FILE_NAME = './wtoken.json'\n",
    "BPE_TOKENIZER_FILE_NAME = './bpetoken.json'\n",
    "\n",
    "BPE_VOCAB_SIZE = 10000\n",
    "WORD_LEVEL_VOCAB_SIZE = 5000\n",
    "\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "SOS_TOKEN = \"[SOS]\"\n",
    "EOS_TOKEN = \"[EOS]\"\n",
    "ALL_TOKENS = [UNK_TOKEN, SOS_TOKEN, EOS_TOKEN, PAD_TOKEN]\n",
    "\n",
    "ALL_TRAINING_DATA = [\n",
    "    './cultural.txt',\n",
    "    './economics.txt',\n",
    "    './politics.txt',\n",
    "    './sports.txt'\n",
    "]\n",
    "\n",
    "LM_TRAINING_DATA = ['./t.txt'] #ALL_TRAINING_DATA[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve6-4WobxrDV"
   },
   "source": [
    "# **Tokenization and Post-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RBbEmvzCxqaZ"
   },
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZERS:\n",
    "    word_tokenizer = Tokenizer(WordLevel(unk_token=UNK_TOKEN))\n",
    "    word_tokenizer.pre_tokenizer = Whitespace()\n",
    "    word_trainer = WordLevelTrainer(vocab_size=WORD_LEVEL_VOCAB_SIZE, special_tokens=ALL_TOKENS)\n",
    "    word_tokenizer.train(ALL_TRAINING_DATA, word_trainer)\n",
    "    word_tokenizer.enable_padding(pad_token=PAD_TOKEN)\n",
    "    word_tokenizer.save(WORD_TOKENIZER_FILE_NAME)\n",
    "else:\n",
    "    word_tokenizer = Tokenizer.from_file(WORD_TOKENIZER_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "E_thOmQuy0lX"
   },
   "outputs": [],
   "source": [
    "if TRAIN_TOKENIZERS:\n",
    "    bpe_tokenizer = Tokenizer(BPE(unk_token=UNK_TOKEN))\n",
    "    bpe_tokenizer.pre_tokenizer = Whitespace()\n",
    "    bpe_trainer = BpeTrainer(vocab_size=BPE_VOCAB_SIZE, special_tokens=ALL_TOKENS)\n",
    "    bpe_tokenizer.train(ALL_TRAINING_DATA, bpe_trainer)\n",
    "    bpe_tokenizer.enable_padding(pad_token=PAD_TOKEN)\n",
    "    bpe_tokenizer.save(BPE_TOKENIZER_FILE_NAME)\n",
    "else:\n",
    "    bpe_tokenizer = Tokenizer.from_file(BPE_TOKENIZER_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dWQNwyOOzBjH"
   },
   "outputs": [],
   "source": [
    "def add_post_processor_to(tokenizer: Tokenizer):\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=f\"{SOS_TOKEN} $0 {EOS_TOKEN}\",\n",
    "        special_tokens=[\n",
    "            (X, tokenizer.token_to_id(X)) for X in [SOS_TOKEN, EOS_TOKEN]\n",
    "        ]\n",
    "    )\n",
    "add_post_processor_to(word_tokenizer)\n",
    "add_post_processor_to(bpe_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjjGhSR82KOv",
    "outputId": "11579885-c826-4a08-939b-c6db71b96c79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenizer: ['[SOS]', 'سلاااااام', 'حالت', 'خوب', 'است', '؟', '[EOS]']\n",
      "BPE Tokenizer: ['[SOS]', 'س', 'لا', 'ا', 'ا', 'ا', 'ا', 'ام', 'حالت', 'خوب', 'است', '؟', '[EOS]']\n"
     ]
    }
   ],
   "source": [
    "sample = 'سلاااااام حالت خوب است؟'\n",
    "print(f'Word Tokenizer: {word_tokenizer.encode(sample).tokens}')\n",
    "print(f'BPE Tokenizer: {bpe_tokenizer.encode(sample).tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P165HXtImeom"
   },
   "source": [
    "# **Dataset Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kKehavNksEYl"
   },
   "outputs": [],
   "source": [
    "class TextDataset():\n",
    "    def __init__(self, corpus_file, tokenizer, length=5, lines_num=None, batch_size=32):\n",
    "        self.length = length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        print('Preparing data...')\n",
    "\n",
    "        # Read the data and get the subset\n",
    "        lines = None\n",
    "        with open(corpus_file, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        if lines_num is not None:\n",
    "            lines = lines[:lines_num]\n",
    "        \n",
    "        # Convert to x, y data\n",
    "        self.x, self.y = [], []\n",
    "        for line in tqdm(lines, bar_format=\"{l_bar}{bar:50}{r_bar}{bar:-10b}\"):\n",
    "            tokens = tokenizer.encode(line).ids\n",
    "            if len(tokens) < length + 1:\n",
    "                continue\n",
    "            for i in range(0, len(tokens) - length):\n",
    "                self.x.append(tokens[i:i+length])\n",
    "                self.y.append(tokens[i+1:i+length+1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def get_batch(self):\n",
    "        # for i in range(int(len(self.x)/self.batch_size) + 1):\n",
    "        #     # x_batch = np.array(self.x[i::int(len(self.x)/self.batch_size) + 1], dtype='int64')\n",
    "        #     # y_batch = np.array(self.y[i::int(len(self.x)/self.batch_size) + 1], dtype='int64')\n",
    "        #     x_batch = np.array(self.x[i:i + self.batch_size], dtype='int64')\n",
    "        #     y_batch = np.array(self.y[i:i + self.batch_size], dtype='int64')\n",
    "        #     yield torch.tensor(x_batch), torch.tensor(y_batch)\n",
    "\n",
    "        start = 0\n",
    "        while (start + self.batch_size) <= len(self.x):\n",
    "            x_batch = np.array(self.x[start:start+self.batch_size], dtype='int64')\n",
    "            y_batch = np.array(self.y[start:start+self.batch_size], dtype='int64')\n",
    "            yield torch.tensor(x_batch), torch.tensor(y_batch)\n",
    "            start = start + self.batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8edc3EW1VQi"
   },
   "source": [
    "# **Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0JbKPcJt1UEO"
   },
   "outputs": [],
   "source": [
    "class LanguageModelLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_layer = nn.Embedding(vocab_size, 100)\n",
    "        self.fc1 = nn.Linear(100, 50)\n",
    "        self.lstm = nn.LSTM(50, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # line_start_pos = (x[:, 0] == 1).nonzero()[:, 0]\n",
    "        # h0, c0 = hidden[0][:, :x.shape[0], :], hidden[1][:, :x.shape[0], :]\n",
    "        # h0[:, line_start_pos, :], c0[:, line_start_pos, :] = 0, 0\n",
    "        # hidden = h0, c0\n",
    "\n",
    "        embedded = self.emb_layer(x)\n",
    "        embedded = self.fc1(embedded.reshape(-1, 100)).reshape(x.shape[0], x.shape[1], -1)\n",
    "        out, hidden = self.lstm(embedded.permute((1, 0, 2)), hidden)\n",
    "        out = out.reshape((-1, hidden_size))\n",
    "        out = self.fc2(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m04QOFDim-rx"
   },
   "source": [
    "# **Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "k3JxpvMMnDdO"
   },
   "outputs": [],
   "source": [
    "def create_h0_state(batch_size, hidden_size):\n",
    "    h0, c0 = np.zeros((1, batch_size, hidden_size), dtype='float32'), np.zeros((1, batch_size, hidden_size), dtype='float32')\n",
    "    hidden = torch.tensor(h0).to(device), torch.tensor(c0).to(device)\n",
    "    return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flowniZqytOv"
   },
   "source": [
    "# **Training Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3gcsubvAWwm2"
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, epochs, train_dataset, device, batch_size=32, save_every=5, path=None, hidden_size=100):\n",
    "    \n",
    "    counter = 0\n",
    "    train_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        h0, c0 = np.zeros((1, batch_size, hidden_size), dtype='float32'), np.zeros((1, batch_size, hidden_size), dtype='float32')\n",
    "        hidden = torch.tensor(h0).to(device), torch.tensor(c0).to(device)\n",
    "\n",
    "        iter = 0\n",
    "        tick = time.time()\n",
    "        for x, y in train_dataset.get_batch():\n",
    "            iter += 1\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "            y_pred, hidden = model(x, hidden)\n",
    "            loss = loss_fn(y_pred, y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss\n",
    "            \n",
    "            counter += 1\n",
    "            print_every = 500\n",
    "            if counter % print_every == 0:\n",
    "                model.eval()\n",
    "                average_loss = train_loss.item() / print_every\n",
    "                print('Epoch {} - Iteration {}: Loss = {:.2f} PP = {:.2f}'.format(epoch + 1, counter,\n",
    "                                                                                  average_loss, np.exp(average_loss)))\n",
    "                train_loss = 0\n",
    "                model.train()\n",
    "        \n",
    "        tock = time.time()\n",
    "        print('Epoch {} finished with {} iters (Duration: {:.2f}s).'.format(epoch+1, iter, tock-tick))\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            torch.save(model.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH6-4knQ1xRs"
   },
   "source": [
    "# **Here We Go** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSFQBddG_1N-",
    "outputId": "0a9e6818-fa45-471c-863c-563c4f94da32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 40000/40000 [00:08<00:00, 4643.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data pairs: 3878205\n",
      "Epoch 1 - Iteration 500: Loss = 5.73 PP = 306.81\n",
      "Epoch 1 - Iteration 1000: Loss = 5.54 PP = 255.52\n",
      "Epoch 1 - Iteration 1500: Loss = 5.59 PP = 266.89\n",
      "Epoch 1 - Iteration 2000: Loss = 5.60 PP = 270.14\n",
      "Epoch 1 - Iteration 2500: Loss = 6.04 PP = 418.11\n",
      "Epoch 1 - Iteration 3000: Loss = 6.01 PP = 407.90\n",
      "Epoch 1 - Iteration 3500: Loss = 6.05 PP = 424.44\n",
      "Epoch 1 - Iteration 4000: Loss = 6.09 PP = 442.75\n",
      "Epoch 1 - Iteration 4500: Loss = 5.96 PP = 386.73\n",
      "Epoch 1 - Iteration 5000: Loss = 5.94 PP = 381.72\n",
      "Epoch 1 - Iteration 5500: Loss = 5.96 PP = 386.76\n",
      "Epoch 1 - Iteration 6000: Loss = 5.94 PP = 379.85\n",
      "Epoch 1 - Iteration 6500: Loss = 5.89 PP = 360.57\n",
      "Epoch 1 - Iteration 7000: Loss = 5.91 PP = 367.87\n",
      "Epoch 1 - Iteration 7500: Loss = 5.93 PP = 374.50\n",
      "Epoch 1 finished with 7574 iters (Duration: 83.33s).\n",
      "Epoch 2 - Iteration 8000: Loss = 5.66 PP = 287.68\n",
      "Epoch 2 - Iteration 8500: Loss = 5.52 PP = 249.04\n",
      "Epoch 2 - Iteration 9000: Loss = 5.57 PP = 261.70\n",
      "Epoch 2 - Iteration 9500: Loss = 5.55 PP = 256.11\n",
      "Epoch 2 - Iteration 10000: Loss = 5.99 PP = 401.10\n",
      "Epoch 2 - Iteration 10500: Loss = 6.00 PP = 404.70\n",
      "Epoch 2 - Iteration 11000: Loss = 6.03 PP = 417.63\n",
      "Epoch 2 - Iteration 11500: Loss = 6.04 PP = 418.69\n",
      "Epoch 2 - Iteration 12000: Loss = 5.93 PP = 374.96\n",
      "Epoch 2 - Iteration 12500: Loss = 5.93 PP = 377.00\n",
      "Epoch 2 - Iteration 13000: Loss = 5.94 PP = 380.08\n",
      "Epoch 2 - Iteration 13500: Loss = 5.93 PP = 374.75\n",
      "Epoch 2 - Iteration 14000: Loss = 5.88 PP = 358.03\n",
      "Epoch 2 - Iteration 14500: Loss = 5.89 PP = 363.08\n",
      "Epoch 2 - Iteration 15000: Loss = 5.92 PP = 374.24\n",
      "Epoch 2 finished with 7574 iters (Duration: 83.38s).\n",
      "Epoch 3 - Iteration 15500: Loss = 5.68 PP = 294.15\n",
      "Epoch 3 - Iteration 16000: Loss = 5.47 PP = 238.38\n",
      "Epoch 3 - Iteration 16500: Loss = 5.55 PP = 256.81\n",
      "Epoch 3 - Iteration 17000: Loss = 5.56 PP = 258.77\n",
      "Epoch 3 - Iteration 17500: Loss = 5.89 PP = 361.49\n",
      "Epoch 3 - Iteration 18000: Loss = 5.98 PP = 394.81\n",
      "Epoch 3 - Iteration 18500: Loss = 6.01 PP = 406.80\n",
      "Epoch 3 - Iteration 19000: Loss = 6.01 PP = 406.57\n",
      "Epoch 3 - Iteration 19500: Loss = 5.93 PP = 376.12\n",
      "Epoch 3 - Iteration 20000: Loss = 5.91 PP = 370.43\n",
      "Epoch 3 - Iteration 20500: Loss = 5.93 PP = 376.23\n",
      "Epoch 3 - Iteration 21000: Loss = 5.91 PP = 367.71\n",
      "Epoch 3 - Iteration 21500: Loss = 5.86 PP = 351.58\n",
      "Epoch 3 - Iteration 22000: Loss = 5.88 PP = 356.53\n",
      "Epoch 3 - Iteration 22500: Loss = 5.91 PP = 366.96\n",
      "Epoch 3 finished with 7574 iters (Duration: 83.38s).\n",
      "Epoch 4 - Iteration 23000: Loss = 5.72 PP = 306.18\n",
      "Epoch 4 - Iteration 23500: Loss = 5.46 PP = 233.97\n",
      "Epoch 4 - Iteration 24000: Loss = 5.54 PP = 254.36\n",
      "Epoch 4 - Iteration 24500: Loss = 5.55 PP = 257.41\n",
      "Epoch 4 - Iteration 25000: Loss = 5.77 PP = 321.01\n",
      "Epoch 4 - Iteration 25500: Loss = 5.96 PP = 389.22\n",
      "Epoch 4 - Iteration 26000: Loss = 5.98 PP = 396.39\n",
      "Epoch 4 - Iteration 26500: Loss = 6.00 PP = 402.15\n",
      "Epoch 4 - Iteration 27000: Loss = 5.92 PP = 373.70\n",
      "Epoch 4 - Iteration 27500: Loss = 5.93 PP = 375.50\n",
      "Epoch 4 - Iteration 28000: Loss = 5.91 PP = 367.17\n",
      "Epoch 4 - Iteration 28500: Loss = 5.90 PP = 364.30\n",
      "Epoch 4 - Iteration 29000: Loss = 5.84 PP = 345.23\n",
      "Epoch 4 - Iteration 29500: Loss = 5.89 PP = 360.36\n",
      "Epoch 4 - Iteration 30000: Loss = 5.86 PP = 352.00\n",
      "Epoch 4 finished with 7574 iters (Duration: 83.41s).\n",
      "Epoch 5 - Iteration 30500: Loss = 5.76 PP = 316.84\n",
      "Epoch 5 - Iteration 31000: Loss = 5.46 PP = 235.29\n",
      "Epoch 5 - Iteration 31500: Loss = 5.51 PP = 245.94\n",
      "Epoch 5 - Iteration 32000: Loss = 5.54 PP = 255.90\n",
      "Epoch 5 - Iteration 32500: Loss = 5.71 PP = 302.50\n",
      "Epoch 5 - Iteration 33000: Loss = 5.95 PP = 382.52\n",
      "Epoch 5 - Iteration 33500: Loss = 5.97 PP = 391.22\n",
      "Epoch 5 - Iteration 34000: Loss = 5.99 PP = 397.94\n",
      "Epoch 5 - Iteration 34500: Loss = 5.93 PP = 376.44\n",
      "Epoch 5 - Iteration 35000: Loss = 5.91 PP = 368.25\n",
      "Epoch 5 - Iteration 35500: Loss = 5.90 PP = 364.73\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-24a540c261a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     train(net, loss_fn, optimizer, 1000, train_dataset, device,\n\u001b[0m\u001b[1;32m     20\u001b[0m           batch_size=batch_size, save_every=save_every, path=path, hidden_size=hidden_size)\n\u001b[1;32m     21\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2158737c3aae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_fn, optimizer, epochs, train_dataset, device, batch_size, save_every, path, hidden_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_every = 5\n",
    "batch_size = 512\n",
    "hidden_size = 50\n",
    "train_mode = True\n",
    "corpus_path = './t.txt'\n",
    "tokenizer = word_tokenizer\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "path = './lstm_{}.pth'.format('word' if tokenizer == word_tokenizer else 'bpe')\n",
    "# path = '/content/drive/MyDrive/Language Modeling (NLP HW3)/lstm_{}.pth'.format('word' if tokenizer == word_tokenizer else 'bpe')\n",
    "\n",
    "net = LanguageModelLSTM(tokenizer.get_vocab_size(), hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_dataset = TextDataset(corpus_path, tokenizer, length=10, lines_num=40000, batch_size=batch_size)\n",
    "print('Number of data pairs:', len(train_dataset))\n",
    "\n",
    "if train_mode:\n",
    "    train(net, loss_fn, optimizer, 1000, train_dataset, device,\n",
    "          batch_size=batch_size, save_every=save_every, path=path, hidden_size=hidden_size)\n",
    "else:\n",
    "    # net.load_state_dict(torch.load(path))\n",
    "    net.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKjRxkVGYFql"
   },
   "source": [
    "# **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "zYtn0sl6rQUt"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, length):\n",
    "    softmax = torch.nn.Softmax()\n",
    "    sentence = []\n",
    "    hidden = create_h0_state(1)\n",
    "    start_token = tokenizer.token_to_id('[SOS]')\n",
    "    print(start_token)\n",
    "    x = np.array([[start_token]], dtype='int64')\n",
    "    model.eval()\n",
    "    for i in range(length):\n",
    "        x = torch.tensor(x).to(device)\n",
    "        out, hidden = model(x, hidden)\n",
    "        hidden = hidden[0].data, hidden[1].data\n",
    "        probs = softmax(out).view(-1)\n",
    "        next_pred = np.random.choice(np.arange(probs.shape[0]), 1, p=probs.detach().cpu().numpy()).item()\n",
    "        sentence.append(next_pred)\n",
    "        x = np.array([[next_pred]], dtype='int64')\n",
    "    sentence = [tokenizer.id_to_token(i) for i in sentence]\n",
    "    sentence = ' '.join(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Sj9qkTYSE9EV"
   },
   "outputs": [],
   "source": [
    "def compute_line_loss(model, tokenizer, line):\n",
    "    tokens = np.array(tokenizer.encode(line).ids, dtype='int64').reshape(1, -1)\n",
    "    x = tokens[:, :-1]\n",
    "    y = tokens[:, 1:].reshape(-1)\n",
    "    length = x.shape[1]\n",
    "    \n",
    "    loss = 0\n",
    "    model.eval()\n",
    "    hidden = create_h0_state(1, hidden_size)\n",
    "    y_pred, hidden = model(torch.tensor(x).to(device), hidden)\n",
    "    loss = F.cross_entropy(y_pred, torch.tensor(y).to(device))\n",
    "    \n",
    "    # L = 10\n",
    "    # for i in range(np.math.ceil(length/L)):\n",
    "    #     y_pred, hidden = model(torch.tensor(x[:, i:i+L]).to(device), hidden)\n",
    "    #     this_loss = F.cross_entropy(y_pred, torch.tensor(y[i:i+L]).to(device))\n",
    "    #     loss += this_loss * y_pred.shape[0]\n",
    "    # loss /= length\n",
    "    return loss, length\n",
    "\n",
    "def compute_corpus_perplexity(corpus_file, num_lines, model, tokenizer):\n",
    "    lines = None\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    lines = lines[:num_lines]\n",
    "    total_loss = 0\n",
    "    total_length = 0\n",
    "    for line in lines:\n",
    "        loss, length = compute_line_loss(model, tokenizer, line)\n",
    "        total_loss += loss * length\n",
    "        total_length += length\n",
    "    total_loss /= total_length\n",
    "    perplexity = torch.exp(total_loss)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KpFNPpjE2U_o",
    "outputId": "ca4b2ee7-0de4-4f15-abec-49cd40ba6446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word level perplexity: 185.31858825683594\n"
     ]
    }
   ],
   "source": [
    "pp = compute_corpus_perplexity('t.txt', 1000, net, word_tokenizer)\n",
    "print('Word level perplexity: {}'.format(pp))\n",
    "\n",
    "# pp = compute_corpus_perplexity('t.txt', 10, net, bpe_tokenizer)\n",
    "# print('bpe level perplexity: {}'.format(pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLGRVY2tWx49"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_language_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
